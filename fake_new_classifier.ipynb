{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten\n",
    "from keras.preprocessing import sequence\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data1 = pd.read_csv(\"../data/Fake and real news dataset/Fake.csv\")\n",
    "true_data1 = pd.read_csv(\"../data/Fake and real news dataset/True.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true_data1.shape\n",
    "#true_data1.head()\n",
    "#checking for invalid entries\n",
    "true_data1.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fake_data1.shape\n",
    "#fake_data1.head()\n",
    "#checking for invalid entries\n",
    "fake_data1.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true_data1[['title', 'text']]\n",
    "#fake_data1[['title', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_counts(titles, top, plt_title):\n",
    "    \n",
    "\n",
    "    #titles = fake_data1['title']\n",
    "    title_words = {}\n",
    "    word = \"\"\n",
    "\n",
    "    for title in titles:\n",
    "        title = title.split(\" \")\n",
    "\n",
    "        for word in title:\n",
    "            word = word.strip()\n",
    "            word = word.strip(\",\")\n",
    "            word = word.strip(\".\")\n",
    "            word = word.strip(\"'\")\n",
    "            word = word.strip(\":\")\n",
    "\n",
    "            if word.isalpha():\n",
    "                if word not in title_words:\n",
    "                    title_words[word] = 1\n",
    "                else:\n",
    "                    title_words[word] += 1\n",
    "\n",
    "    sorted_words = sorted(title_words.items(), key = lambda x: x[1], reverse = True)\n",
    "    plotting_words = sorted_words[0:top]\n",
    "    words_x = [tup[0] for tup in plotting_words]\n",
    "    counts_y = [tup[1] for tup in plotting_words]\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.bar(range(len(plotting_words)), counts_y)\n",
    "    plt.xticks(range(len(plotting_words)), words_x)\n",
    "    plt.xlabel(\"Words\")\n",
    "    plt.ylabel(\"Counts\")\n",
    "    plt.title(plt_title)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_titles = true_data1['title']\n",
    "fake_titles = fake_data1['title']\n",
    "plot_top_counts(fake_titles, 10, \"Fake news words\")\n",
    "plot_top_counts(true_titles, 10, \"Real news words\")\n",
    "#write comment about hwo this data set is heavility based of articles from the US politiacal..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the fake and true datasets and adding a new column indicating where its fake or true\n",
    "\n",
    "is_fake = [1 for x in range(len(fake_data1))]\n",
    "fake_data1['is_fake'] = is_fake\n",
    "\n",
    "is_fake = [0 for x in range(len(true_data1))]\n",
    "true_data1['is_fake'] = is_fake\n",
    "\n",
    "frames = [true_data1, fake_data1]\n",
    "merged1 = pd.concat(frames)\n",
    "\n",
    "merged1.describe\n",
    "merged1 = sk.utils.shuffle(merged1)\n",
    "merged1.head()\n",
    "\n",
    "#splitting data into training and tests sets\n",
    "split = 0.2\n",
    "train_tles, test_tles, train_tles_res, test_tles_res = train_test_split(merged1['title'], merged1['is_fake'], test_size=split, random_state=12)\n",
    "train_txts, test_txts, train_txts_res, test_txts_res = train_test_split(merged1['text'], merged1['is_fake'], test_size=split, random_state=12)\n",
    "\n",
    "#print(len(train_txts))\n",
    "#print(len(train_res_txts))\n",
    "#print(len(test_tles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a word dictionaries where each word is assigned a unique number \n",
    "\n",
    "#tokenizer for titles set\n",
    "n_words = 10000\n",
    "tokenizer_tles = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words = n_words, filters='!\"\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}\\'~\\t\\n.', lower=True,\n",
    "    split=' ', char_level=False,  oov_token=True)\n",
    "\n",
    "tokenizer_tles.fit_on_texts(train_tles)\n",
    "train_tles_seqs = tokenizer_tles.texts_to_sequences(train_tles)\n",
    "\n",
    "max_tle_len = max(len(x) for x in train_tles_seqs)\n",
    "\n",
    "train_tles_seqs = sequence.pad_sequences(train_tles_seqs, maxlen=max_tle_len)\n",
    "test_tles_seqs = tokenizer_tles.texts_to_sequences(test_tles)\n",
    "test_tles_seqs = sequence.pad_sequences(test_tles_seqs, maxlen=max_tle_len)\n",
    "\n",
    "\n",
    "\n",
    "#tokenizer for texts set\n",
    "tokenizer_txts = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words = n_words, filters='!\"\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}\\'~\\t\\n.', lower=True,\n",
    "    split=' ', char_level=False,  oov_token=True)\n",
    "\n",
    "tokenizer_txts.fit_on_texts(train_txts)\n",
    "train_txts_seqs = tokenizer_txts.texts_to_sequences(train_txts)\n",
    "\n",
    "max_txt_len = max(len(x) for x in train_txts_seqs)\n",
    "\n",
    "train_txts_seqs = sequence.pad_sequences(train_txts_seqs, maxlen=max_txt_len)\n",
    "test_txts_seqs = tokenizer_txts.texts_to_sequences(test_txts)\n",
    "test_txts_seqs = sequence.pad_sequences(test_txts_seqs, maxlen=max_txt_len)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(max_tle_len)\n",
    "\n",
    "print(max_txt_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(tokenizer_tles.word_index.values()))\n",
    "print(len(train_tles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building model for analysing titles\n",
    "#experiment with this \n",
    "inp_dim = max(tokenizer_tles.word_index.values()) + 1\n",
    "out_dim = 24\n",
    "\n",
    "model_tles = Sequential()\n",
    "model_tles.add(Embedding(inp_dim, out_dim, input_length=max_tle_len))\n",
    "model_tles.add(Flatten())\n",
    "model_tles.add(Dense(8, activation='relu'))\n",
    "model_tles.add(Dense(8, activation='relu'))\n",
    "model_tles.add(Dense(1, activation='sigmoid'))\n",
    "model_tles.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "print(model_tles.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_tles_seqs))\n",
    "print(len(train_tles_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting model for predicting if a title is part of a fake news article or not\n",
    "hist_tles = model_tles.fit(train_tles_seqs, train_tles_res, validation_data=(test_tles_seqs, test_tles_res), epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist_tles.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "sns.set()\n",
    "acc = hist_tles.history['accuracy']\n",
    "val = hist_tles.history['val_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, '-', label='Training accuracy')\n",
    "plt.plot(epochs, val, ':', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.plot()\n",
    "\n",
    "scores_tles = model_tles.evaluate(test_tles_seqs, test_tles_res, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores_tles[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building model for analysing texts\n",
    "inp_dim = max(tokenizer_txts.word_index.values()) + 1\n",
    "out_dim = 32\n",
    "\n",
    "model_txts = Sequential()\n",
    "model_txts.add(Embedding(inp_dim, out_dim, input_length=max_txt_len))\n",
    "model_txts.add(Flatten())\n",
    "model_txts.add(Dense(16, activation='relu'))\n",
    "model_txts.add(Dense(16, activation='relu'))\n",
    "model_txts.add(Dense(1, activation='sigmoid'))\n",
    "model_txts.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "print(model_txts.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting model for predicting a text/article body is fake or not\n",
    "hist_txts = model_txts.fit(train_txts_seqs, train_txts_res, validation_data=(test_txts_seqs, test_txts_res), epochs=3, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "sns.set()\n",
    "acc = hist_txts.history['accuracy']\n",
    "val = hist_txts.history['val_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, '-', label='Training accuracy')\n",
    "plt.plot(epochs, val, ':', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.plot()\n",
    "\n",
    "scores_tles = model_txts.evaluate(test_txts_seqs, test_txts_res, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores_tles[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
